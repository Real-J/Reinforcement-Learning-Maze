# Reinforcement Learning Maze ğŸ†ğŸš€

This project implements a **Reinforcement Learning (RL) agent** that learns to navigate a maze using **Q-learning**. The agent interacts with a **Tkinter-based grid environment**, avoiding obstacles and reaching the goal while improving its performance over multiple training episodes.

---

## ğŸ¯ **Project Overview**
- **Environment:** A 4x4 grid-based maze.
- **Agent:** Moves in four directions (`up`, `down`, `left`, `right`).
- **Obstacles (`hells`)**: Black squares that penalize the agent with a **-1 reward**.
- **Goal (`paradise`)**: A yellow circle that rewards the agent with a **+1 reward**.
- **Ground (`neutral states`)**: All other cells provide **0 reward**.
- **Q-Learning:** The agent learns an optimal policy through trial and error using an evolving Q-table.

---

## ğŸ“‚ **Project Structure**
```
ğŸ“¦ RL-Maze
 â”£ ğŸ“œ maze_env.py     # Maze environment (Tkinter visualization & environment logic)
 â”£ ğŸ“œ RL_brain.py     # Q-learning agent (learning algorithm & Q-table management)
 â”£ ğŸ“œ main.py         # Training loop and agent-environment interaction
 â”£ ğŸ“œ README.md       # Project documentation
```

---

## ğŸš€ **How to Run**
### **1ï¸âƒ£ Install Dependencies**
Ensure you have **Python 3** installed. Install required dependencies using:
```bash
pip install numpy pandas tk
```

### **2ï¸âƒ£ Run the Program**
To start training the agent and visualize the maze environment, run:
```bash
python main.py
```
This will launch the **maze environment**, where the agent will attempt to learn the optimal path through reinforcement learning.

---

## ğŸ”¬ **How It Works**
### **1ï¸âƒ£ Training Process**
- The agent starts at the **red square**.
- It explores the maze using the **Îµ-greedy policy**:
  - **Exploits** the best-known action (90% of the time).
  - **Explores** new actions (10% of the time).
- The agent updates the **Q-table** using the Q-learning formula to improve future decision-making.
- After multiple episodes, it learns the **optimal path** to reach the goal efficiently.

### **2ï¸âƒ£ Q-Learning Explanation**
Q-learning is a model-free reinforcement learning algorithm used to find the best action to take in a given state. It follows the **Bellman equation**:
```
Q(s, a) â† Q(s, a) + Î± [r + Î³ max Q(s', a') - Q(s, a)]
```
Where:
- **`Q(s, a)`** â†’ The expected utility of taking action `a` in state `s`.
- **`Î±` (learning rate)** â†’ Controls how much new information overrides old knowledge.
- **`r` (reward)** â†’ Immediate reward received after taking action `a`.
- **`Î³` (discount factor)** â†’ Balances the importance of future rewards (0 â‰¤ Î³ â‰¤ 1).
- **`max Q(s', a')`** â†’ The highest expected reward from the next state `s'`.

#### **How Learning Works**
1. The agent starts with a **Q-table** initialized to zero.
2. It selects an action using an **Îµ-greedy policy** (choosing between exploitation and exploration).
3. The agent performs the action, observes the **reward** and next state.
4. The **Q-table** is updated using the formula above.
5. Over time, the agent learns the **optimal policy**â€”the sequence of actions that maximize the expected cumulative reward.
6. The **policy improves** as more training episodes refine the action-value estimates.

#### **Advantages of Q-Learning**
- **Model-Free:** The agent does not need prior knowledge of the environment.
- **Guaranteed Convergence:** Given sufficient exploration, Q-learning converges to the optimal policy.
- **Handles Stochastic Environments:** Can adapt to changes in rewards and state transitions.
- **Off-Policy Learning:** Learns the optimal policy regardless of the agent's action selection strategy.

#### **Challenges of Q-Learning**
- **Exploration-Exploitation Dilemma:** Balancing between exploring new actions and exploiting known good actions.
- **Slow Convergence:** Can require many episodes for complex environments.
- **Curse of Dimensionality:** Large state-action spaces can make learning impractical without optimizations.

#### **Enhancements to Q-Learning**
- **Decay Îµ Over Time:** Gradually shift from exploration to exploitation.
- **Experience Replay:** Store past experiences and reuse them to stabilize learning.
- **Function Approximation:** Use **Neural Networks (Deep Q-Networks, DQNs)** to generalize learning.
- **Double Q-Learning:** Reduce overestimation of action values to improve stability.

---

## ğŸ”¥ **Features**
âœ… **Q-learning with exploration & exploitation**  
âœ… **Graphical visualization using Tkinter**  
âœ… **Dynamic interaction with the environment**  
âœ… **Customizable training episodes**  
âœ… **Adjustable learning parameters**  
âœ… **Step-by-step Q-table updates**
âœ… **Discussion of Q-learning advantages & challenges**

---

## ğŸ› ï¸ **Possible Enhancements**
- **Larger & More Complex Mazes** ğŸ—ï¸
- **Adaptive Exploration Rate (`Îµ` Decay)** ğŸ“‰
- **Save & Load Q-Table for Continued Training** ğŸ’¾
- **Deep Q-Networks (DQN) for More Complex Environments** ğŸ§ 
- **Obstacle & Maze Customization** ğŸ› ï¸
- **Multiple agents with collaborative learning** ğŸ‘¥

---

## ğŸ“œ **License**
This project is open-source and free to use under the MIT License.

