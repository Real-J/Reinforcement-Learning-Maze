# Reinforcement Learning Maze ğŸ†ğŸš€

This project implements a **Reinforcement Learning (RL) agent** that learns to navigate a maze using **Q-learning**. The agent interacts with a **Tkinter-based grid environment**, avoiding obstacles and reaching the goal while improving its performance over multiple training episodes.

---

## ğŸ¯ **Project Overview**
- **Environment:** A 4x4 grid-based maze.
- **Agent:** Moves in four directions (`up`, `down`, `left`, `right`).
- **Obstacles (`hells`)**: Black squares that penalize the agent with a **-1 reward**.
- **Goal (`paradise`)**: A yellow circle that rewards the agent with a **+1 reward**.
- **Ground (`neutral states`)**: All other cells provide **0 reward**.
- **Q-Learning:** The agent learns an optimal policy through trial and error using an evolving Q-table.

---

## ğŸ“‚ **Project Structure**
```
ğŸ“¦ RL-Maze
 â”£ ğŸ“œ maze_env.py     # Maze environment (Tkinter visualization & environment logic)
 â”£ ğŸ“œ RL_brain.py     # Q-learning agent (learning algorithm & Q-table management)
 â”£ ğŸ“œ main.py         # Training loop and agent-environment interaction
 â”£ ğŸ“œ README.md       # Project documentation
```

---

## ğŸš€ **How to Run**
### **1ï¸âƒ£ Install Dependencies**
Ensure you have **Python 3** installed. Then, install the required dependencies using:
```bash
pip install numpy pandas tk
```
This will install:
- `numpy`: For numerical operations and array manipulations.
- `pandas`: To manage and update the Q-table efficiently.
- `tkinter`: For rendering the graphical user interface (GUI) of the maze.

### **2ï¸âƒ£ Run the Program**
To start training the agent and visualize the maze environment, follow these steps:
1. Open a terminal or command prompt.
2. Navigate to the project directory:
   ```bash
   cd path/to/RL-Maze
   ```
3. Run the main script:
   ```bash
   python main.py
   ```

### **3ï¸âƒ£ Understanding the Output**
- The **Tkinter window** will open, displaying the 4x4 maze.
- The **red square (agent)** moves based on its learned Q-values.
- The agent will explore, learn, and gradually find the shortest path to the goal.
- Once training is completed, the Q-table will have optimized values for navigation.
- The training process will print updates in the terminal, showing rewards and learning progress.

### **4ï¸âƒ£ Modify Training Parameters**
You can adjust parameters such as:
- **Number of episodes** (increase to improve learning):
  ```python
  for episode in range(500):  # Change from 100 to 500
  ```
- **Learning rate (`Î±`)** and **discount factor (`Î³`)** in `RL_brain.py`:
  ```python
  def __init__(self, actions, learning_rate=0.1, reward_decay=0.9, e_greedy=0.9):
  ```
- **Exploration rate (`Îµ`) decay**:
  ```python
  self.epsilon *= 0.995  # Gradually decrease exploration over time
  ```

### **5ï¸âƒ£ Stop and Restart Training**
- Close the Tkinter window to stop training.
- Run `python main.py` again to restart learning.
- To **save and reload the Q-table**, implement a file-saving mechanism in `RL_brain.py`.

---

## ğŸ”¬ **How It Works**
### **1ï¸âƒ£ Training Process**
- The agent starts at the **red square**.
- It explores the maze using the **Îµ-greedy policy**:
  - **Exploits** the best-known action (90% of the time).
  - **Explores** new actions (10% of the time).
- The agent updates the **Q-table** using the Q-learning formula to improve future decision-making.
- After multiple episodes, it learns the **optimal path** to reach the goal efficiently.

### **2ï¸âƒ£ Q-Learning Explanation**
Q-learning is a model-free reinforcement learning algorithm used to find the best action to take in a given state. It follows the **Bellman equation**:
```
Q(s, a) â† Q(s, a) + Î± [r + Î³ max Q(s', a') - Q(s, a)]
```
Where:
- **`Q(s, a)`** â†’ The expected utility of taking action `a` in state `s`.
- **`Î±` (learning rate)** â†’ Controls how much new information overrides old knowledge.
- **`r` (reward)** â†’ Immediate reward received after taking action `a`.
- **`Î³` (discount factor)** â†’ Balances the importance of future rewards (0 â‰¤ Î³ â‰¤ 1).
- **`max Q(s', a')`** â†’ The highest expected reward from the next state `s'`.

---

## ğŸ”¥ **Features**
âœ… **Q-learning with exploration & exploitation**  
âœ… **Graphical visualization using Tkinter**  
âœ… **Dynamic interaction with the environment**  
âœ… **Customizable training episodes**  
âœ… **Adjustable learning parameters**  
âœ… **Step-by-step Q-table updates**



---

## ğŸ”¬ **How It Works**
### **1ï¸âƒ£ Training Process**
- The agent starts at the **red square**.
- It explores the maze using the **Îµ-greedy policy**:
  - **Exploits** the best-known action (90% of the time).
  - **Explores** new actions (10% of the time).
- The agent updates the **Q-table** using the Q-learning formula to improve future decision-making.
- After multiple episodes, it learns the **optimal path** to reach the goal efficiently.

### **2ï¸âƒ£ Q-Learning Explanation**
Q-learning is a model-free reinforcement learning algorithm used to find the best action to take in a given state. It follows the **Bellman equation**:
```
Q(s, a) â† Q(s, a) + Î± [r + Î³ max Q(s', a') - Q(s, a)]
```
Where:
- **`Q(s, a)`** â†’ The expected utility of taking action `a` in state `s`.
- **`Î±` (learning rate)** â†’ Controls how much new information overrides old knowledge.
- **`r` (reward)** â†’ Immediate reward received after taking action `a`.
- **`Î³` (discount factor)** â†’ Balances the importance of future rewards (0 â‰¤ Î³ â‰¤ 1).
- **`max Q(s', a')`** â†’ The highest expected reward from the next state `s'`.

#### **How Learning Works**
1. The agent starts with a **Q-table** initialized to zero.
2. It selects an action using an **Îµ-greedy policy** (choosing between exploitation and exploration).
3. The agent performs the action, observes the **reward** and next state.
4. The **Q-table** is updated using the formula above.
5. Over time, the agent learns the **optimal policy**â€”the sequence of actions that maximize the expected cumulative reward.
6. The **policy improves** as more training episodes refine the action-value estimates.

#### **Advantages of Q-Learning**
- **Model-Free:** The agent does not need prior knowledge of the environment.
- **Guaranteed Convergence:** Given sufficient exploration, Q-learning converges to the optimal policy.
- **Handles Stochastic Environments:** Can adapt to changes in rewards and state transitions.
- **Off-Policy Learning:** Learns the optimal policy regardless of the agent's action selection strategy.

#### **Challenges of Q-Learning**
- **Exploration-Exploitation Dilemma:** Balancing between exploring new actions and exploiting known good actions.
- **Slow Convergence:** Can require many episodes for complex environments.
- **Curse of Dimensionality:** Large state-action spaces can make learning impractical without optimizations.

#### **Enhancements to Q-Learning**
- **Decay Îµ Over Time:** Gradually shift from exploration to exploitation.
- **Experience Replay:** Store past experiences and reuse them to stabilize learning.
- **Function Approximation:** Use **Neural Networks (Deep Q-Networks, DQNs)** to generalize learning.
- **Double Q-Learning:** Reduce overestimation of action values to improve stability.

---

## ğŸ”¥ **Features**
âœ… **Q-learning with exploration & exploitation**  
âœ… **Graphical visualization using Tkinter**  
âœ… **Dynamic interaction with the environment**  
âœ… **Customizable training episodes**  
âœ… **Adjustable learning parameters**  
âœ… **Step-by-step Q-table updates**
âœ… **Discussion of Q-learning advantages & challenges**

---

## ğŸ› ï¸ **Possible Enhancements**
- **Larger & More Complex Mazes** ğŸ—ï¸
- **Adaptive Exploration Rate (`Îµ` Decay)** ğŸ“‰
- **Save & Load Q-Table for Continued Training** ğŸ’¾
- **Deep Q-Networks (DQN) for More Complex Environments** ğŸ§ 
- **Obstacle & Maze Customization** ğŸ› ï¸
- **Multiple agents with collaborative learning** ğŸ‘¥

---

##ğŸ˜ Result

![Input Image](c.jpeg)


## ğŸ“œ **License**
This project is open-source and free to use under the MIT License.

